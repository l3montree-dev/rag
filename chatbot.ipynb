{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1494d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "from collections.abc import Sequence\n",
    "from typing import cast\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "161879df",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PATH_DIR = os.getenv(\"DOCS_PATH\")\n",
    "CHUNK_SIZE = 1000\n",
    "OVERLAP_SIZE = 50\n",
    "MODEL_EMBEDDING = \"mistral-embed\"\n",
    "MODEL_GENERATION = \"mistral-small-2506\" #mistral small 3.2\n",
    "BATCH_SIZE = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9050df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the docs and return as a single string\n",
    "def read_docs() -> str:\n",
    "    # search for all .md files in the directory\n",
    "    docs : str = \"\"\n",
    "    for root, _, files in os.walk(str(PATH_DIR)):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                with open(os.path.join(root, file), \"r\") as f:\n",
    "                    docs += f.read() + \"\\n\"\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f345e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Markdown files in /Users/amelieartmann/Documents/devguard-chatbot: 73\n"
     ]
    }
   ],
   "source": [
    "# small sanity check to see if the number of .md files is correct\n",
    "def print_number_of_md_files() -> None:\n",
    "    num : int = 0\n",
    "    for _,_, files in os.walk(str(PATH_DIR)):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                num += 1\n",
    "    print(f\"Number of Markdown files in {PATH_DIR}: {num}\")\n",
    "print_number_of_md_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6f28eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # chunk given string into smaller chunks of chosen size with overlap\n",
    "def chunking(docs: str) -> list[str]:\n",
    "    chunks : list = []\n",
    "    for i in range(0, len(docs), CHUNK_SIZE - OVERLAP_SIZE):\n",
    "        chunks.append(docs[i:i+CHUNK_SIZE])\n",
    "    return chunks \"\"\"\n",
    "\n",
    "# do chunking without splitting up words\n",
    "def better_chunking(docs: str) -> list[str]:\n",
    "    chunks : list[str] = []\n",
    "    start : int = 0\n",
    "    while start < len(docs):\n",
    "        end : int = start + CHUNK_SIZE\n",
    "        if end >= len(docs):\n",
    "            chunks.append(docs[start:])\n",
    "            break\n",
    "        else:\n",
    "            # find last space before end\n",
    "            last_space : int = docs.rfind(\" \", start, end)\n",
    "            # if no space found, just split at end\n",
    "            if last_space == -1:\n",
    "                last_space = end\n",
    "            chunks.append(docs[start:last_space])\n",
    "            start = last_space - OVERLAP_SIZE\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c6edf65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"API_KEY\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a83cec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embeddings for a list of chunks, return a list of embeddings\n",
    "def get_embeddings(chunks: list[str]) -> list[list[float]]:\n",
    "    embeddings: list[list[float]] = []\n",
    "    # call the api with batches to avoid hitting the rate limit\n",
    "    for i in range(0, len(chunks), BATCH_SIZE):\n",
    "        batch = chunks[i:i + BATCH_SIZE]\n",
    "        response = client.embeddings.create(\n",
    "            model=MODEL_EMBEDDING,\n",
    "            inputs=batch\n",
    "        )\n",
    "        for data_item in response.data:\n",
    "            embedding: Sequence[float] = cast(Sequence[float], data_item.embedding)\n",
    "            embeddings.append(list(embedding))\n",
    "    return embeddings\n",
    "\n",
    "# get embedding for a single chunk of text\n",
    "def text_embedding(chunk: str) -> list[float]:\n",
    "    # call the mistral api to get the embedding for the given text\n",
    "    response = client.embeddings.create(\n",
    "        model=MODEL_EMBEDDING,\n",
    "        inputs=[chunk]\n",
    "    )\n",
    "    embedding = cast(Sequence[float], response.data[0].embedding)\n",
    "    return list(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d184841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of better chunks: 168\n"
     ]
    }
   ],
   "source": [
    "# first read the docs\n",
    "docs : str = read_docs()\n",
    "\n",
    "# then chunk the docs\n",
    "chunks : list[str] = better_chunking(docs)\n",
    "print(f\"Number of better chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "fa6d6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"embedding_db\",\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    host=\"localhost\",\n",
    "    port = 5432\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0edc1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the chunks and their embeddings into the database\n",
    "cur = conn.cursor()\n",
    "embeddings = get_embeddings(chunks)\n",
    "for chunk, embedding in zip(chunks, embeddings):\n",
    "    cur.execute(\n",
    "        \"INSERT INTO documents (content, embedding) VALUES (%s, %s)\",\n",
    "        (chunk, embedding)\n",
    "    )\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fd377793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(query:str, k:int=3) -> list[tuple[str,float]] | None:\n",
    "    \"\"\"\n",
    "    Supported distance functions (for non-binary vectors):\n",
    "    <+> L1 distance\n",
    "    <-> L2 distance\n",
    "    <=> cosine distance -> use 1 - cosine distance\n",
    "    <#> inner product -> multiply by -1, since else it negative inner product\n",
    "    \"\"\"\n",
    "    query_embedding : list[float] = text_embedding(query)\n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "        SELECT content,\n",
    "            1 -(embedding <=> %s::vector) AS distance\n",
    "        FROM documents\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT %s;\n",
    "        \"\"\", (query_embedding, query_embedding, k))\n",
    "        results: list[tuple[str, float]] = cur.fetchall()\n",
    "    \n",
    "        # Optional: print results\n",
    "        #for content, similarity in results:\n",
    "        #    print(content, similarity)\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(\"SQL ERROR:\", e)\n",
    "        conn.rollback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c2ce5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, context: list[tuple[str, float]]) -> str:\n",
    "    # format context\n",
    "    context_text = \"\\n\\n\".join(\n",
    "        f\"- {content}\" for content, _ in context\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Use ONLY the following context to answer the question.\n",
    "    If the answer cannot be answered using the context, say you don't know.\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    message= [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    \"\"\"\n",
    "        Toggling the safe prompt will prepend your messages with the following system prompt:\n",
    "        Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
    "    \"\"\"\n",
    "    response = client.chat.complete(\n",
    "        model=MODEL_GENERATION,\n",
    "        messages=message,\n",
    "        safe_prompt=True,\n",
    "        temperature=0.0 # no randomness, since we want the same answer for the same question and context\n",
    "    )\n",
    "    return str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4deadf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DevGuard is a tool built by developers for developers to simplify vulnerability management. It aims to integrate security seamlessly into the software development lifecycle, making security practices accessible and efficient for everyone, regardless of their security expertise.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "query = \"What is DevGuard?\"\n",
    "top_k_results = retrieve_top_k(query, k=5)\n",
    "\n",
    "if top_k_results is not None:\n",
    "    test_query = generate_response(query, top_k_results)\n",
    "    print(test_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
